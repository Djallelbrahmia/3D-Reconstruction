{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module imports necessary libraries and defines the intrinsic camera parameters, \n",
    "loads a set of images, and implements a function to match keypoints between two images using the SIFT algorithm.\n",
    "\n",
    "Imports:\n",
    "- numpy: For numerical operations and array handling.\n",
    "- cv2: OpenCV library for computer vision tasks.\n",
    "\n",
    "Camera Intrinsic Parameters:\n",
    "- K: A 3x3 matrix representing the intrinsic parameters of the camera, including focal lengths and optical center.\n",
    "\n",
    "Image Loading:\n",
    "- image_paths: A list of file paths for the images to be processed, generated using a formatted string for file naming.\n",
    "\n",
    "Function: get_matches(path1, path2)\n",
    "-------------------------------------\n",
    "This function takes in the paths of two images, performs the following operations:\n",
    "1. Reads the images from the specified paths.\n",
    "2. Checks if the images are loaded correctly; if not, it logs an error message.\n",
    "3. Uses the SIFT algorithm to detect keypoints and compute their descriptors.\n",
    "4. Sets up the FLANN-based matcher to find the best matches between the descriptors of the two images.\n",
    "5. Applies Lowe's ratio test to filter out weak matches.\n",
    "6. Collects matched keypoints' coordinates and their corresponding colors from both images.\n",
    "\n",
    "Parameters:\n",
    "- path1 (str): The file path for the first image.\n",
    "- path2 (str): The file path for the second image.\n",
    "\n",
    "Returns:\n",
    "- matched_points_a (np.ndarray): Coordinates of matched keypoints in the first image.\n",
    "- matched_points_b (np.ndarray): Coordinates of matched keypoints in the second image.\n",
    "- total_colors_a (np.ndarray): Color values of matched keypoints in the first image.\n",
    "- total_colors_b (np.ndarray): Color values of matched keypoints in the second image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Camera intrinsic parameters\n",
    "K = np.array([\n",
    "    [1270.95, 0.00032, 518.57],\n",
    "    [0, 1272.75, 393.43],\n",
    "    [0, 0, 1]\n",
    "])\n",
    "\n",
    "# Loading images \n",
    "image_paths = [f'./Assets/{i:04d}.jpg' for i in range(8)]\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "\n",
    "def get_matches(path1,path2):\n",
    "    img1 = cv.imread(path1)\n",
    "    img2 = cv.imread(path2)\n",
    "        # Initialize lists to store colors\n",
    "    total_colors_a = []  # Initialize as lists\n",
    "    total_colors_b = []\n",
    "\n",
    "    # Ensure the images are read correctly\n",
    "    if img1 is None or img2 is None:\n",
    "        print(f\"Error reading images\")\n",
    "        return \n",
    "    \n",
    "    kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "    \n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)  # or pass empty dictionary\n",
    "    flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "    \n",
    "    # Need to draw only good matches, so create a mask\n",
    "    matchesMask = [[0, 0] for _ in range(len(matches))]\n",
    "    \n",
    "    # Ratio test as per Lowe's paper\n",
    "    for j, (m, n) in enumerate(matches):\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            matchesMask[j] = [1, 0]\n",
    "\n",
    "    # Lists to store coordinates of matched keypoints\n",
    "    matched_points_a = []\n",
    "    matched_points_b = []\n",
    "\n",
    "    # Iterate through the matches and their mask\n",
    "    for j, (m, n) in enumerate(matches):\n",
    "        if matchesMask[j][0] == 1:  # Only consider good matches\n",
    "            # Get the coordinates from keypoints\n",
    "            point_a = kp1[m.queryIdx].pt  # Coordinates in Image A\n",
    "            point_b = kp2[m.trainIdx].pt  # Coordinates in Image B\n",
    "            \n",
    "            # Append the coordinates to the lists\n",
    "            matched_points_a.append(point_a)  # (x, y) coordinates in Image A\n",
    "            matched_points_b.append(point_b)  # (x, y) coordinates in Image B\n",
    "\n",
    "            # Extract colors from the images based on matched feature points\n",
    "            color_a = img1[int(point_a[1]), int(point_a[0])]  # Color from Image A\n",
    "            color_b = img2[int(point_b[1]), int(point_b[0])]  # Color from Image B\n",
    "            \n",
    "            # Store colors in total colors lists\n",
    "            total_colors_a.append(color_a.tolist())  # Convert to list before appending\n",
    "            total_colors_b.append(color_b.tolist())  # Convert to list before appending\n",
    "    return np.array(matched_points_a) , np.array(matched_points_b) , np.array(total_colors_a) , np.array(total_colors_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell initializes transformation matrices and computes the initial camera poses based on the camera's intrinsic parameters. \n",
    "It also retrieves matched feature points and their corresponding colors from two consecutive images.\n",
    "\n",
    "Key Variables:\n",
    "- transform_matrix_0: A 3x4 transformation matrix representing the initial pose of the first camera. \n",
    "                      It is initialized to the identity matrix for the rotational part and zero for the translational part.\n",
    "- transform_matrix_1: A 3x4 empty matrix that will be populated later to represent the pose of the second camera.\n",
    "- pose_0: A 3x4 matrix representing the camera pose of the first image, computed by multiplying the intrinsic matrix K with transform_matrix_0.\n",
    "- pose_1: A 3x4 empty matrix that will later hold the computed pose of the second image.\n",
    "- feature_0: 2D coordinates of matched keypoints in the first image retrieved by the get_matches function.\n",
    "- feature_1: 2D coordinates of matched keypoints in the second image retrieved by the get_matches function.\n",
    "- color_a: Color values of matched keypoints from the first image.\n",
    "- color_b: Color values of matched keypoints from the second image.\n",
    "\n",
    "Operations:\n",
    "1. Initializes the transformation matrices to prepare for camera pose estimation.\n",
    "2. Calculates the pose of the first camera using the intrinsic parameters.\n",
    "3. Calls the get_matches function to obtain matched keypoints and their colors from the first two images in the dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_matrix_0 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "transform_matrix_1 = np.empty((3, 4))\n",
    "    \n",
    "pose_0 = np.matmul(K, transform_matrix_0)\n",
    "pose_1 = np.empty((3, 4)) \n",
    "feature_0, feature_1,color_a,color_b = get_matches(image_paths[0], image_paths[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell computes the essential matrix from the matched feature points between the first and second images,\n",
    "recovers the camera pose from this essential matrix, and updates the transformation matrices accordingly.\n",
    "\n",
    "Key Variables:\n",
    "- essential_matrix: The computed essential matrix that encodes the relative pose between the two cameras based on the matched feature points.\n",
    "- em_mask: A mask indicating which matches are inliers, obtained from the RANSAC method used to estimate the essential matrix.\n",
    "- rot_matrix: The rotation component of the camera pose, extracted from the essential matrix.\n",
    "- tran_matrix: The translation component of the camera pose, extracted from the essential matrix.\n",
    "- transform_matrix_1: A 3x4 transformation matrix representing the pose of the second camera, updated based on the recovered rotation and translation.\n",
    "- pose_1: A 3x4 matrix representing the camera pose of the second image, calculated by multiplying the intrinsic matrix K with transform_matrix_1.\n",
    "\n",
    "Operations:\n",
    "1. Computes the essential matrix using the RANSAC method and refines the matched feature points to keep only inliers.\n",
    "2. Recovers the rotation and translation matrices from the essential matrix.\n",
    "3. Updates the transformation matrix for the second camera pose using the recovered rotation and translation.\n",
    "4. Calculates the final pose of the second camera by applying the intrinsic camera parameters to the updated transformation matrix.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "essential_matrix, em_mask = cv.findEssentialMat(feature_0, feature_1, K, method=cv.RANSAC, prob=0.999, threshold=0.4, mask=None)\n",
    "feature_0 = feature_0[em_mask.ravel() == 1]\n",
    "feature_1 = feature_1[em_mask.ravel() == 1]\n",
    "color_a=color_a[em_mask.ravel() == 1]\n",
    "color_b=color_b[em_mask.ravel() == 1]\n",
    "\n",
    "_, rot_matrix, tran_matrix, em_mask = cv.recoverPose(essential_matrix, feature_0, feature_1,K)\n",
    "feature_0 = feature_0[em_mask.ravel() > 0]\n",
    "feature_1 = feature_1[em_mask.ravel() > 0]\n",
    "color_a=color_a[em_mask.ravel() > 0]\n",
    "color_b=color_b[em_mask.ravel() > 0]\n",
    "transform_matrix_1[:3, :3] = np.matmul(rot_matrix, transform_matrix_0[:3, :3])\n",
    "transform_matrix_1[:3, 3] = transform_matrix_0[:3, 3] + np.matmul(transform_matrix_0[:3, :3], tran_matrix.ravel())\n",
    "\n",
    "pose_1 = np.matmul(K, transform_matrix_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Triangulates 3D points from two sets of 2D image points and their corresponding projection matrices.\n",
    "\n",
    "Parameters:\n",
    "- point_2d_1 (numpy.ndarray): A 2D array of shape (N, 2) containing the (x, y) coordinates of points from the first image.\n",
    "- point_2d_2 (numpy.ndarray): A 2D array of shape (N, 2) containing the (x, y) coordinates of points from the second image.\n",
    "- projection_matrix_1 (numpy.ndarray): A 3x4 projection matrix corresponding to the first camera, used for projecting 3D points to 2D.\n",
    "- projection_matrix_2 (numpy.ndarray): A 3x4 projection matrix corresponding to the second camera, used for projecting 3D points to 2D.\n",
    "\n",
    "Returns:\n",
    "- projection_matrix_1.T (numpy.ndarray): The transposed projection matrix of the first camera.\n",
    "- projection_matrix_2.T (numpy.ndarray): The transposed projection matrix of the second camera.\n",
    "- point_cloud (numpy.ndarray): A 4xN array of homogeneous coordinates of the triangulated 3D points. The points are normalized by dividing by the fourth coordinate to obtain Cartesian coordinates.\n",
    "\n",
    "Note:\n",
    "The triangulated points are computed using the OpenCV function `cv.triangulatePoints`, which combines the two sets of 2D points and their respective projection matrices to produce the 3D point cloud.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulation(point_2d_1, point_2d_2, projection_matrix_1, projection_matrix_2):\n",
    "        \n",
    "        '''\n",
    "        Triangulates 3d points from 2d vectors and projection matrices\n",
    "        returns projection matrix of first camera, projection matrix of second camera, point cloud \n",
    "        '''\n",
    "\n",
    "\n",
    "        pt_cloud = cv.triangulatePoints(point_2d_1, point_2d_2, projection_matrix_1.T, projection_matrix_2.T)\n",
    "        return projection_matrix_1.T, projection_matrix_2.T, (pt_cloud / pt_cloud[3])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nEstimates the pose of an object from 3D-2D point correspondences using the RANSAC algorithm.\\n\\nParameters:\\n- obj_point (numpy.ndarray): A 3D array of shape (N, 3) containing the object points in the world coordinate system.\\n- image_point (numpy.ndarray): A 2D array of shape (N, 2) containing the corresponding 2D points in the image coordinate system.\\n- K (numpy.ndarray): A 3x3 camera intrinsic matrix that defines the camera parameters.\\n- rot_vector (numpy.ndarray): A 1D array representing the initial rotation vector of shape (3,).\\n\\nReturns:\\n- rot_matrix (numpy.ndarray): A 3x3 rotation matrix derived from the estimated rotation vector.\\n- tran_vector (numpy.ndarray): A 3x1 translation vector that describes the position of the object in the camera coordinate system.\\n- image_point (numpy.ndarray): The filtered array of 2D image points that correspond to inliers found by RANSAC.\\n- obj_point (numpy.ndarray): The filtered array of 3D object points corresponding to the inliers.\\n- rot_vector (numpy.ndarray): The rotation vector corresponding to the inliers.\\n\\nNote:\\nThe function utilizes OpenCV's `cv.solvePnPRansac` to estimate the pose, robustly handling outliers. It also converts the rotation vector to a rotation matrix using `cv.Rodrigues`.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Estimates the pose of an object from 3D-2D point correspondences using the RANSAC algorithm.\n",
    "\n",
    "Parameters:\n",
    "- obj_point (numpy.ndarray): A 3D array of shape (N, 3) containing the object points in the world coordinate system.\n",
    "- image_point (numpy.ndarray): A 2D array of shape (N, 2) containing the corresponding 2D points in the image coordinate system.\n",
    "- K (numpy.ndarray): A 3x3 camera intrinsic matrix that defines the camera parameters.\n",
    "- rot_vector (numpy.ndarray): A 1D array representing the initial rotation vector of shape (3,)\n",
    "\n",
    "Returns:\n",
    "- rot_matrix (numpy.ndarray): A 3x3 rotation matrix derived from the estimated rotation vector.\n",
    "- tran_vector (numpy.ndarray): A 3x1 translation vector that describes the position of the object in the camera coordinate system.\n",
    "- image_point (numpy.ndarray): The filtered array of 2D image points that correspond to inliers found by RANSAC.\n",
    "- obj_point (numpy.ndarray): The filtered array of 3D object points corresponding to the inliers.\n",
    "- rot_vector (numpy.ndarray): The rotation vector corresponding to the inliers.\n",
    "\n",
    "Note:\n",
    "The function utilizes OpenCV's `cv.solvePnPRansac` to estimate the pose, robustly handling outliers. It also converts the rotation vector to a rotation matrix using `cv.Rodrigues`.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PnP( obj_point, image_point , K, rot_vector) :\n",
    "        \n",
    "        '''\n",
    "        Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.\n",
    "        returns rotational matrix, translational matrix, image points, object points, rotational vector\n",
    "        '''\n",
    "\n",
    "\n",
    "        _, rot_vector_calc, tran_vector, inlier = cv.solvePnPRansac(obj_point, image_point, K, np.zeros((5, 1), dtype=np.float32), cv.SOLVEPNP_ITERATIVE)\n",
    "        rot_matrix, _ = cv.Rodrigues(rot_vector_calc)\n",
    "\n",
    "        if inlier is not None:\n",
    "            image_point = image_point[inlier[:, 0]]\n",
    "            obj_point = obj_point[inlier[:, 0]]\n",
    "            rot_vector = rot_vector[inlier[:, 0]]\n",
    "        return rot_matrix, tran_vector, image_point, obj_point, rot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the reprojection error, which is the distance between the projected 3D points and the corresponding 2D points in the image.\n",
    "\n",
    "Parameters:\n",
    "- obj_points (numpy.ndarray): A 3D array of shape (N, 3) representing the object points in the world coordinate system.\n",
    "- image_points (numpy.ndarray): A 2D array of shape (N, 2) containing the corresponding 2D points in the image coordinate system.\n",
    "- transform_matrix (numpy.ndarray): A 4x4 transformation matrix that contains the rotation and translation information.\n",
    "- K (numpy.ndarray): A 3x3 camera intrinsic matrix that defines the camera parameters.\n",
    "- homogenity (int): A flag to indicate if the input object points are in homogeneous coordinates; `1` if true, `0` otherwise.\n",
    "\n",
    "Returns:\n",
    "- total_error (float): The average reprojection error normalized by the number of projected points.\n",
    "- obj_points (numpy.ndarray): The original object points passed to the function, possibly in converted homogeneous coordinates.\n",
    "\n",
    "Note:\n",
    "The function projects the 3D object points onto the image plane using the provided transformation and intrinsic camera parameters, then calculates the reprojection error using the L2 norm.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojection_error(obj_points, image_points, transform_matrix, K, homogenity):\n",
    "\n",
    "        '''\n",
    "        Calculates the reprojection error ie the distance between the projected points and the actual points.\n",
    "        returns total error, object points\n",
    "        '''\n",
    "        rot_matrix = transform_matrix[:3, :3]\n",
    "        tran_vector = transform_matrix[:3, 3]\n",
    "        rot_vector, _ = cv.Rodrigues(rot_matrix)\n",
    "        if homogenity == 1:\n",
    "            obj_points = cv.convertPointsFromHomogeneous(obj_points.T)\n",
    "        image_points_calc, _ = cv.projectPoints(obj_points, rot_vector, tran_vector, K, None)\n",
    "        image_points_calc = np.float32(image_points_calc[:, 0, :])\n",
    "        total_error = cv.norm(image_points_calc, np.float32(image_points.T) if homogenity == 1 else np.float32(image_points), cv.NORM_L2)\n",
    "        return total_error / len(image_points_calc), obj_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finds the common points between two pairs of image points: image 1 and image 2, and image 2 and image 3.\n",
    "\n",
    "Parameters:\n",
    "- image_points_1 (numpy.ndarray): A 2D array of shape (N, 2) containing the points from image 1.\n",
    "- image_points_2 (numpy.ndarray): A 2D array of shape (M, 2) containing the points from image 2.\n",
    "- image_points_3 (numpy.ndarray): A 2D array of shape (P, 2) containing the points from image 3.\n",
    "\n",
    "Returns:\n",
    "- cm_points_1 (numpy.ndarray): An array of indices of common points in image 1 that correspond to image 2.\n",
    "- cm_points_2 (numpy.ndarray): An array of indices of common points in image 2 that correspond to image 1.\n",
    "- mask_array_1 (numpy.ndarray): A 2D array of shape (K, 2) containing the points from image 2 that are not common to image 1.\n",
    "- mask_array_2 (numpy.ndarray): A 2D array of shape (L, 2) containing the points from image 3 that are not common to image 2.\n",
    "\n",
    "Note:\n",
    "This function identifies common keypoints between the first two images and between the second and third images by comparing the coordinates. It creates masks for points in the second and third images that do not have corresponding points in the first and second images, respectively.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_points( image_points_1, image_points_2, image_points_3) :\n",
    "        '''\n",
    "        Finds the common points between image 1 and 2 , image 2 and 3\n",
    "        returns common points of image 1-2, common points of image 2-3, mask of common points 1-2 , mask for common points 2-3 \n",
    "        '''\n",
    "        cm_points_1 = []\n",
    "        cm_points_2 = []\n",
    "        for i in range(image_points_1.shape[0]):\n",
    "            a = np.where(image_points_2 == image_points_1[i, :])\n",
    "            if a[0].size != 0:\n",
    "                cm_points_1.append(i)\n",
    "                cm_points_2.append(a[0][0])\n",
    "\n",
    "        mask_array_1 = np.ma.array(image_points_2, mask=False)\n",
    "        mask_array_1.mask[cm_points_2] = True\n",
    "        mask_array_1 = mask_array_1.compressed()\n",
    "        mask_array_1 = mask_array_1.reshape(int(mask_array_1.shape[0] / 2), 2)\n",
    "\n",
    "        mask_array_2 = np.ma.array(image_points_3, mask=False)\n",
    "        mask_array_2.mask[cm_points_2] = True\n",
    "        mask_array_2 = mask_array_2.compressed()\n",
    "        mask_array_2 = mask_array_2.reshape(int(mask_array_2.shape[0] / 2), 2)\n",
    "        print(\" Shape New Array\", mask_array_1.shape, mask_array_2.shape)\n",
    "        return np.array(cm_points_1), np.array(cm_points_2), mask_array_1, mask_array_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Triangulates 3D points from 2D correspondences between two camera poses.\n",
    "\n",
    "Parameters:\n",
    "- pose_0 (numpy.ndarray): The projection matrix for the first camera, shape (3, 4).\n",
    "- pose_1 (numpy.ndarray): The projection matrix for the second camera, shape (3, 4).\n",
    "- feature_0 (numpy.ndarray): A 2D array of points from the first image, shape (N, 2).\n",
    "- feature_1 (numpy.ndarray): A 2D array of points from the second image, shape (M, 2).\n",
    "\n",
    "Returns:\n",
    "- points_3d (numpy.ndarray): A 3D array of triangulated points, shape (K, 3), where K is the number of 3D points.\n",
    "\n",
    "Notes:\n",
    "1. The triangulation function estimates the 3D coordinates of points based on their 2D projections in two images using the provided camera poses.\n",
    "2. The output `points_3d` is converted from homogeneous coordinates to regular 3D coordinates. The conversion process normalizes the points so that they are represented in Cartesian coordinates (x, y, z).\n",
    "3. The final shape of `points_3d` is (K, 3), where K corresponds to the number of successfully triangulated points.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, points_3d = triangulation(pose_0, pose_1, feature_0, feature_1)\n",
    "points_3d = cv.convertPointsFromHomogeneous(points_3d.T)\n",
    "points_3d = points_3d[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iteratively computes the 3D structure from a sequence of images using triangulation and pose estimation.\n",
    "\n",
    "The code iterates through a series of images, computing and refining the 3D point cloud and associated color data from consecutive image pairs.\n",
    "\n",
    "Parameters:\n",
    "- pose_array (numpy.ndarray): Array to store the camera poses as flattened vectors.\n",
    "- total_points (numpy.ndarray): Array containing the current 3D point cloud, initially populated with previous triangulated points.\n",
    "- total_colors (numpy.ndarray): Array holding the colors corresponding to the 3D points.\n",
    "\n",
    "Loop through the images:\n",
    "1. For each pair of consecutive images, match features using the `get_matches` function.\n",
    "2. Triangulate the 3D points from the matched features using the current and previous poses.\n",
    "3. Convert the 3D points from homogeneous coordinates to Cartesian coordinates.\n",
    "4. Identify common points between the matched features of the consecutive images.\n",
    "5. Estimate the camera pose using the PnP algorithm with RANSAC to reduce the effect of outliers.\n",
    "6. Compute the reprojection error to evaluate the accuracy of the estimated points.\n",
    "7. Triangulate common points again for better accuracy.\n",
    "8. Store the camera poses, 3D points, and colors of the points for visualization and further processing.\n",
    "\n",
    "Returns:\n",
    "- None: The function updates the `pose_array`, `total_points`, and `total_colors` arrays in place.\n",
    "\n",
    "Notes:\n",
    "1. The use of tqdm provides a visual progress bar for the iteration, indicating how many images have been processed.\n",
    "2. The reprojection error helps monitor the accuracy of the 3D reconstruction process.\n",
    "3. The code assumes that `K`, `pose_0`, and `pose_1` are defined and valid camera matrices for the corresponding images.\n",
    "4. The loop iterates from the first image to the second last image in the `image_paths`, ensuring that pairs of images are processed.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:03<00:18,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape New Array (440, 2) (440, 2)\n",
      "(490, 3)\n",
      "(490, 2)\n",
      "(5, 1)\n",
      "Reprojection Error:  1.683977159738542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:09<00:19,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape New Array (951, 2) (951, 2)\n",
      "(565, 3)\n",
      "(565, 2)\n",
      "(5, 1)\n",
      "Reprojection Error:  1.0456964589718885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:14<00:14,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape New Array (522, 2) (522, 2)\n",
      "(837, 3)\n",
      "(837, 2)\n",
      "(5, 1)\n",
      "Reprojection Error:  9.10381923267001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:18<00:09,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape New Array (710, 2) (710, 2)\n",
      "(655, 3)\n",
      "(655, 2)\n",
      "(5, 1)\n",
      "Reprojection Error:  7.6619861787075525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:23<00:04,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape New Array (850, 2) (850, 2)\n",
      "(920, 3)\n",
      "(920, 2)\n",
      "(5, 1)\n",
      "Reprojection Error:  0.633565757669806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:28<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shape New Array (711, 2) (711, 2)\n",
      "(1043, 3)\n",
      "(1043, 2)\n",
      "(5, 1)\n",
      "Reprojection Error:  53.73645788390992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pose_array = K.ravel()\n",
    "total_points = points_3d\n",
    "total_colors =color_b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(image_paths)-2)):\n",
    "        image = cv.imread(image_paths[i + 1])  # Read the current next image\n",
    "        feature_0, feature_1, _, _ = get_matches(image_paths[i], image_paths[i+1])\n",
    "        feature_0, feature_1, points_3d = triangulation(pose_0, pose_1, feature_0, feature_1)\n",
    "        feature_1 = feature_1.T\n",
    "        features_cur, features_2, _, _ = get_matches(image_paths[i+1], image_paths[i+2])\n",
    "        points_3d = cv.convertPointsFromHomogeneous(points_3d.T)\n",
    "        points_3d = points_3d[:, 0, :]\n",
    "        cm_points_0, cm_points_1, cm_mask_0, cm_mask_1 = common_points(feature_1, features_cur, features_2)\n",
    "        \n",
    "        cm_points_2 = features_2[cm_points_1]\n",
    "        cm_points_cur = features_cur[cm_points_1]\n",
    "        rot_matrix, tran_matrix, cm_points_2, points_3d, cm_points_cur = PnP(\n",
    "                    points_3d[cm_points_0], cm_points_2, K, \n",
    "                     cm_points_cur\n",
    "                )\n",
    "        transform_matrix_1 = np.hstack((rot_matrix, tran_matrix))\n",
    "        pose_2 = np.matmul(K, transform_matrix_1)\n",
    "\n",
    "                # Calculate reprojection error\n",
    "        error, points_3d = reprojection_error(points_3d, cm_points_2, transform_matrix_1, K, homogenity=0)\n",
    "\n",
    "                # Triangulate common points between poses\n",
    "        cm_mask_0, cm_mask_1, points_3d = triangulation(pose_1, pose_2, cm_mask_0, cm_mask_1)\n",
    "        error, points_3d = reprojection_error(points_3d, cm_mask_1, transform_matrix_1, K, homogenity=1)\n",
    "        print(\"Reprojection Error: \", error)\n",
    "\n",
    "                # Collect 3D points and color information\n",
    "        pose_array = np.hstack((pose_array, pose_2.ravel()))\n",
    "        total_points = np.vstack((total_points, points_3d[:, 0, :]))\n",
    "        points_left = np.array(cm_mask_1, dtype=np.int32)\n",
    "        color_vector = np.array([image[l[1], l[0]] for l in points_left.T])\n",
    "        total_colors = np.vstack((total_colors, color_vector))\n",
    "        pose_0 = np.copy(pose_1)\n",
    "        pose_1 = np.copy(pose_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Generates a .ply file for visualizing the 3D point cloud.\n",
    "\n",
    "    # Parameters:\n",
    "    # - path (str): The directory path where the .ply file will be saved.\n",
    "    # - point_cloud (numpy.ndarray): A 3D array of shape (N, 3) representing the 3D points.\n",
    "    # - colors (numpy.ndarray): A 2D array of shape (N, 3) representing the RGB colors for each point, in the range [0, 1].\n",
    "\n",
    "    # The function performs the following steps:\n",
    "    # 1. Reshapes and scales the 3D point cloud.\n",
    "    # 2. Converts the colors to 8-bit unsigned integers (0-255) suitable for .ply format.\n",
    "    # 3. Combines the point coordinates and colors into a single array.\n",
    "    # 4. Centers the points by subtracting the mean position.\n",
    "    # 5. Applies a distance threshold to filter out points that are too far from the center.\n",
    "    # 6. Constructs the .ply file header with the appropriate format specifications.\n",
    "    # 7. Creates a 'res' directory within the specified path if it does not already exist.\n",
    "    # 8. Writes the point cloud data to the .ply file.\n",
    "\n",
    "    # Returns:\n",
    "    # - None: The function saves the output .ply file to the specified directory.\n",
    "\n",
    "    # Notes:\n",
    "    # - The scaling factor of 200 is applied to the points; adjust this value based on the desired size in the visualization.\n",
    "    # - The distance threshold is calculated as the mean distance plus an offset of 300, which helps in filtering the points based on their spatial distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4679, 3)\n",
      "(4679, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def to_ply(path, point_cloud, colors):\n",
    "    '''\n",
    "    Generates the .ply which can be used to open the point cloud\n",
    "    '''\n",
    "    # Reshape and scale the point cloud\n",
    "    out_points = point_cloud.reshape(-1, 3) * 200  # Scale points if needed\n",
    "\n",
    "    # Convert the colors to the correct format\n",
    "    # Ensure the colors are scaled to 0-255\n",
    "    out_colors = (colors.reshape(-1, 3) * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "    verts = np.hstack([out_points, out_colors])\n",
    "\n",
    "    # Center the points and apply a distance threshold\n",
    "    mean = np.mean(verts[:, :3], axis=0)\n",
    "    scaled_verts = verts[:, :3] - mean\n",
    "    dist = np.sqrt(scaled_verts[:, 0] ** 2 + scaled_verts[:, 1] ** 2 + scaled_verts[:, 2] ** 2)\n",
    "    indx = np.where(dist < np.mean(dist) + 300)\n",
    "    verts = verts[indx]\n",
    "\n",
    "    ply_header = '''ply\n",
    "    format ascii 1.0\n",
    "    element vertex %(vert_num)d\n",
    "    property float x\n",
    "    property float y\n",
    "    property float z\n",
    "    property uchar red\n",
    "    property uchar green\n",
    "    property uchar blue\n",
    "    end_header\n",
    "    '''\n",
    "\n",
    "    # Create the 'res' directory if it doesn't exist\n",
    "    output_dir = os.path.join(path, 'res')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the output ply file\n",
    "    output_file = os.path.join(output_dir, 'point_cloud.ply')  # Naming the file point_cloud.ply\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(ply_header % dict(vert_num=len(verts)))\n",
    "        np.savetxt(f, verts, '%f %f %f %d %d %d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ply('./Assets', total_points, total_colors)  # Using './Assets' as the base path for saving\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
